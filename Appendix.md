# Appendix

### Exploratory Data Analysis (EDA)

Before building predictive models, it is essential to develop a thorough understanding of the dataset. In this section, we conduct Exploratory Data Analysis (EDA) to examine the structure, distribution, and relationships among the features. This includes analyzing summary statistics, identifying patterns or trends, checking for missing or inconsistent values, and visualizing key variables. EDA helps us uncover important insights about the clinical characteristics of the patients, guides feature engineering decisions, and ensures that the dataset is suitable for downstream modeling.

### Preprocessing and Feature  Engineering

We performed minimal preprocessing because the Heart Failure Clinical Records dataset is already clean and contains no missing values. Our work focused on standardizing variable names through a custom cleaning function, applying light feature selection when needed for specific analyses, and scaling continuous variables with StandardScaler for models sensitive to feature magnitude. For PCA, we applied a PowerTransformer to address skewed clinical variables and improve variance stabilization. No new features were engineered, and no data augmentation was used, as synthetic modification of clinical data could distort medically meaningful relationships.

### Regression Analysis

Regression analysis was explored in two phases—first using linear regression to predict the binary death_event variable, and later applying it to the continuous period_of_follow_up_days variable. For both attempts, we implemented standard scaling, polynomial feature expansion with grid-searched degrees, ridge regression for regularization, cross-validation, and forward feature selection. The binary regression model performed poorly overall, and ridge regularization slightly degraded performance; in both cases, the optimal polynomial degree was 1, indicating no benefit from added model complexity. Because the models failed to achieve meaningful predictive accuracy, we did not pursue feature-importance analysis, as any inferred relationships would have been unreliable. When applied to the continuous follow-up-days variable, linear regression performed even worse, reinforcing that linear methods were not well-suited to the structure of this dataset. Overall, regression analysis showed that the relationships in the data are not effectively captured by linear models, even with regularization or feature expansion.

### Logistic Regression

Logistic regression was applied by first standardizing all continuous features and then fitting a binary classifier using death_event_binary as the response variable. Model evaluation began with a default 0.5 decision threshold, after which we analyzed the ROC curve—specifically the false-positive rate, true-positive rate, and associated thresholds—to identify a more optimal cutoff. This process yielded a threshold of 0.391, which improved classification balance; all evaluation metrics were recalculated using this updated boundary. We generated ROC curves, computed AUC, accuracy, error rate, true positive and true negative rates, and recall, and then performed cross-validation to obtain average AUC and accuracy across folds. While logistic regression produced strong results and demonstrated that the dataset is amenable to linear decision boundaries, we did not extend the analysis to feature-importance interpretation, as this was not part of the project goals. Regularization was not required, as the baseline model already performed well and showed no indications of overfitting.

### KNN, Decision Trees, or Random Forest

For classification, we applied both K-Nearest Neighbors (KNN) and Random Forest models. KNN was trained on standardized data, with hyperparameter tuning to identify the optimal number of neighbors (k = 23). Model performance was evaluated using the confusion matrix, accuracy, error rate, true positive and true negative rates, ROC curves, AUC, and 5-fold cross-validation to compute mean AUC and accuracy. Random Forest was trained without standardization and evaluated using similar metrics, including F1 score, ROC curves, AUC, and cross-validation for mean performance. Among the methods tested, Random Forest performed best, delivering the highest accuracy, balanced classification rates, and consistent AUC across folds. Its ensemble structure allowed it to capture nonlinear interactions and feature dependencies in the dataset, making it particularly well-suited for predicting mortality in patients with heart failure.

Random Forest worked best for our dataset, achieving a mean AUC of 0.905 and a mean accuracy of 0.8353 across 5-fold cross-validation. Its ensemble structure allowed it to capture nonlinear interactions and complex relationships among clinical features, providing robust and stable predictions. This made it particularly well-suited for the problem of predicting mortality in heart failure patients, where interactions between variables like age, ejection fraction, and comorbidities are important and cannot be fully captured by simpler models.

### PCA and Clustering

PCA and clustering were applied to explore underlying structure in the heart failure dataset. For PCA, the data was first transformed using a PowerTransformer to stabilize variance, then decomposed to obtain principal components (U, D, V). Variance was relatively evenly distributed across the first nine components, with a cumulative explained variance of 0.85; scatter plots and scree plots were generated but revealed no clearly dominant directions or meaningful separations. For clustering, the data was standardized and analyzed using both K-Means and Hierarchical Clustering, evaluating performance with within-cluster sum of squares, silhouette score, Rand score, and adjusted Rand score. The same clustering analysis was repeated on PCA-transformed data. Overall, clustering produced reasonably consistent subgroup structures, but PCA did not reveal additional structure or improve interpretability. While these methods provided insight into data relationships, they were primarily exploratory and did not directly enhance predictive modeling for mortality.

### Neural Network

We implemented a feedforward neural network using PyTorch to perform binary classification on the heart failure dataset. The data was converted to Torch tensors and standardized. The network consisted of linear layers with ReLU activations and dropout for regularization. During training, we used binary cross-entropy with logits as the loss function, along with an optimizer and learning-rate scheduler. Training included forward and backward propagation, mini-batching, validation, and early stopping. Cross-validation was applied to monitor loss and accuracy across folds, with results visualized using line plots. However, each fold showed clear evidence of overfitting, suggesting the network was memorizing the training data rather than learning generalizable patterns. Given the small dataset and limited feature complexity, we concluded that neural networks were not suitable for this problem and did not provide meaningful predictive performance.

### Hyperparameter Tuning Example

An example of hyperparameter tuning was applied to optimize model performance, with a clear example in K-Nearest Neighbors (KNN). Initially, we used a default k = 3, but then systematically evaluated k values from 1 to 25 using 5-fold cross-validation on the training set. For each k, we computed the mean accuracy across folds and selected the value with the highest score. This process identified k = 23 as the optimal number of neighbors, which improved classification metrics compared to the initial setting. Standard scaling was applied beforehand to ensure that feature magnitudes did not bias the distance calculations in KNN. Similar tuning strategies were applied wherever model parameters could impact performance.